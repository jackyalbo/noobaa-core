name: Hadoop S3A NC Tests
on: [workflow_dispatch]

jobs:
  hadoop-s3a-nc-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    concurrency:
      group: ${{ github.workflow }}-${{ github.ref }}
      cancel-in-progress: true
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Build NooBaa image
        run: |
          set -euo pipefail
          make noobaa

      - name: Start NooBaa NC endpoint
        run: |
          set -euo pipefail
          docker network create s3a-test
          docker run -d --name noobaa-nc --network s3a-test --user 0 noobaa \
            bash -lc '
              set -euo pipefail
              mkdir -p /etc/noobaa.conf.d /nsfs/buckets
              printf "%s\n" \
                "{" \
                "  \"ALLOW_HTTP\": true," \
                "  \"ENDPOINT_PORT\": 6001" \
                "}" \
                > /etc/noobaa.conf.d/config.json
              /usr/local/bin/node /root/node_modules/noobaa-core/src/cmd/manage_nsfs.js account add \
                --config_root /etc/noobaa.conf.d \
                --name s3a-user --uid 0 --gid 0 --new_buckets_path /nsfs/buckets
              mkdir -p /nsfs/buckets/s3a-test
              /usr/local/bin/node /root/node_modules/noobaa-core/src/cmd/manage_nsfs.js bucket add \
                --config_root /etc/noobaa.conf.d \
                --name s3a-test --owner s3a-user --path /nsfs/buckets/s3a-test
              exec /usr/local/bin/node /root/node_modules/noobaa-core/src/cmd/nsfs.js /nsfs/buckets \
                --config_root /etc/noobaa.conf.d
            '

      - name: Wait for endpoint
        run: |
          set -euo pipefail
          for _ in $(seq 1 60); do
            if docker exec noobaa-nc bash -lc "nc -z localhost 6001"; then
              exit 0
            fi
            sleep 2
          done
          echo "NooBaa endpoint did not start in time" >&2
          docker logs noobaa-nc || true
          exit 1

      - name: Fetch S3A credentials
        run: |
          set -euo pipefail
          account_json="$(docker exec -u 0 noobaa-nc \
            /usr/local/bin/node /root/node_modules/noobaa-core/src/cmd/manage_nsfs.js account status \
            --config_root /etc/noobaa.conf.d \
            --name s3a-user --show_secrets)"
          access_key="$(echo "${account_json}" | jq -r '.response.reply.access_keys[0].access_key')"
          secret_key="$(echo "${account_json}" | jq -r '.response.reply.access_keys[0].secret_key')"
          echo "::add-mask::${access_key}"
          echo "::add-mask::${secret_key}"
          {
            echo "S3A_ACCESS_KEY=${access_key}"
            echo "S3A_SECRET_KEY=${secret_key}"
          } >> "${GITHUB_ENV}"

      - name: Run Hadoop S3A tests
        run: |
          set -euo pipefail
          set +e
          mkdir -p logs
          docker run --rm --network s3a-test \
            -e AWS_ACCESS_KEY_ID="${S3A_ACCESS_KEY}" \
            -e AWS_SECRET_ACCESS_KEY="${S3A_SECRET_KEY}" \
            -e S3A_ENDPOINT="noobaa-nc:6001" \
            -v "${PWD}/logs:/logs" \
            maven:3.9.6-eclipse-temurin-11 \
            bash -lc '
              set -euo pipefail
              apt-get update -y
              apt-get install -y git
              git clone --depth 1 --branch rel/release-3.3.6 https://github.com/apache/hadoop.git /tmp/hadoop
              mkdir -p /tmp/hadoop/hadoop-tools/hadoop-aws/src/test/resources
              printf "%s\n" \
                "<configuration>" \
                "  <property>" \
                "    <name>fs.s3a.endpoint</name>" \
                "    <value>${S3A_ENDPOINT}</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.connection.ssl.enabled</name>" \
                "    <value>false</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.path.style.access</name>" \
                "    <value>true</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.aws.credentials.provider</name>" \
                "    <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.access.key</name>" \
                "    <value>${AWS_ACCESS_KEY_ID}</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.secret.key</name>" \
                "    <value>${AWS_SECRET_ACCESS_KEY}</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.region</name>" \
                "    <value>us-east-1</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>test.fs.s3a.name</name>" \
                "    <value>s3a://s3a-test/</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>test.fs.s3a.bucket</name>" \
                "    <value>s3a-test</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.s3a.bucket</name>" \
                "    <value>s3a-test</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>fs.contract.test.fs.s3a</name>" \
                "    <value>s3a://s3a-test/</value>" \
                "  </property>" \
                "  <property>" \
                "    <name>test.fs.s3a.encryption.enabled</name>" \
                "    <value>false</value>" \
                "  </property>" \
                "</configuration>" \
                > /tmp/hadoop/hadoop-tools/hadoop-aws/src/test/resources/auth-keys.xml
              cd /tmp/hadoop
              mvn_cmd="mvn -pl hadoop-tools/hadoop-aws -DskipTests=false -DskipITs=false -Dit.test=ITestS3A* -Dtest=TestS3A* verify"
              echo "Running: ${mvn_cmd}"
              set +e
              eval "${mvn_cmd}"
              mvn_status=$?
              set -e
              python - <<'"'"'PY'"'"'
import os
import xml.etree.ElementTree as ET

report_dirs = [
    "/tmp/hadoop/hadoop-tools/hadoop-aws/target/surefire-reports",
    "/tmp/hadoop/hadoop-tools/hadoop-aws/target/failsafe-reports",
]
failures = 0
errors = 0
tests = 0
skipped = 0
failing_tests = []

for report_dir in report_dirs:
    if not os.path.isdir(report_dir):
        continue
    for name in os.listdir(report_dir):
        if not name.startswith("TEST-") or not name.endswith(".xml"):
            continue
        path = os.path.join(report_dir, name)
        try:
            tree = ET.parse(path)
            root = tree.getroot()
            failures += int(root.attrib.get("failures", 0))
            errors += int(root.attrib.get("errors", 0))
            tests += int(root.attrib.get("tests", 0))
            skipped += int(root.attrib.get("skipped", 0))
            for testcase in root.findall(".//testcase"):
                for tag in ("failure", "error"):
                    elem = testcase.find(tag)
                    if elem is not None:
                        classname = testcase.attrib.get("classname", "unknown")
                        testname = testcase.attrib.get("name", "unknown")
                        message = (elem.attrib.get("message") or "").strip()
                        entry = f"{classname}#{testname} [{tag}]"
                        if message:
                            entry += f" - {message}"
                        failing_tests.append(entry)
        except Exception:
            pass

print(f"Parsed reports: tests={tests} failures={failures} errors={errors} skipped={skipped}")
with open("/logs/failing-tests.txt", "w") as fh:
    for entry in failing_tests:
        fh.write(entry + "\n")
exit(1 if (failures or errors) else 0)
PY
              report_status=$?
              if [ "${mvn_status}" -ne 0 ] || [ "${report_status}" -ne 0 ]; then
                exit 1
              fi
            '
          status=$?
          set -e
          {
            echo "| Check | Result |"
            echo "| --- | --- |"
            if [ "${status}" -eq 0 ]; then
              echo "| Hadoop S3A tests | Success |"
            else
              echo "| Hadoop S3A tests | Failure (exit ${status}) |"
            fi
          } >> "${GITHUB_STEP_SUMMARY}"
          if [ -s "logs/failing-tests.txt" ]; then
            {
              echo ""
              echo "### Failing Test Cases"
              echo ""
              while IFS= read -r line; do
                echo "- ${line}"
              done < "logs/failing-tests.txt"
            } >> "${GITHUB_STEP_SUMMARY}"
          fi
          exit "${status}"
